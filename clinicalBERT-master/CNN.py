import os
import numpy as np
import pandas as pd
import gensim
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_curve, auc

# -------------------------------
# 1. å®šä¹‰æ¨¡å‹ï¼šTextCNN
# -------------------------------
class TextCNNClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_labels=1, embedding_matrix=None, filter_sizes=[3,4,5], num_filters=100):
        super(TextCNNClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        if embedding_matrix is not None:
            self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))
            self.embedding.weight.requires_grad = False

        self.convs = nn.ModuleList([
            nn.Conv1d(in_channels=embed_dim, out_channels=num_filters, kernel_size=fs)
            for fs in filter_sizes
        ])

        self.dropout = nn.Dropout(0.1)
        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_labels)

    def forward(self, input_ids, labels=None):
        embeds = self.embedding(input_ids)  # [batch_size, seq_len, embed_dim]
        x = embeds.permute(0, 2, 1)  # [batch_size, embed_dim, seq_len]
        conv_outputs = [torch.relu(conv(x)) for conv in self.convs]
        pooled_outputs = [torch.max(c, dim=2)[0] for c in conv_outputs]
        cat = torch.cat(pooled_outputs, dim=1)
        cat = self.dropout(cat)
        logits = self.fc(cat)
        if labels is not None:
            loss_fn = nn.BCEWithLogitsLoss()
            loss = loss_fn(logits, labels.float().view(-1, 1))
            return loss, logits
        else:
            return logits

# -------------------------------
# 2. æ•°æ®é›†å®šä¹‰
# -------------------------------
class ClinicalTextDataset(Dataset):
    def __init__(self, csv_file, word2idx, max_seq_length=512):
        self.data = pd.read_csv(csv_file)
        self.texts = self.data["TEXT"].astype(str).tolist()
        self.labels = self.data["Label"].values
        self.word2idx = word2idx
        self.max_seq_length = max_seq_length

    def __len__(self):
        return len(self.texts)

    def tokenize(self, text):
        return text.strip().split()

    def numericalize(self, tokens):
        return [self.word2idx.get(token, self.word2idx["<UNK>"]) for token in tokens]

    def pad_sequence(self, seq):
        if len(seq) < self.max_seq_length:
            seq = seq + [self.word2idx["<PAD>"]] * (self.max_seq_length - len(seq))
        else:
            seq = seq[:self.max_seq_length]
        return seq

    def __getitem__(self, idx):
        text = self.texts[idx]
        tokens = self.tokenize(text)
        indices = self.numericalize(tokens)
        indices = self.pad_sequence(indices)
        label = self.labels[idx]
        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)

# -------------------------------
# 3. ä¸»å‡½æ•°
# -------------------------------
def main():
    # è·¯å¾„å’Œå‚æ•°
    train_csv = "/mnt/nvme2/yyc/clinicalbert/dead/data40k/train.csv"
    val_csv = "/mnt/nvme2/yyc/clinicalbert/dead/data40k/val.csv"
    test_csv = "/mnt/nvme2/yyc/clinicalbert/dead/data40k/test.csv"
    wv_model_path = "/mnt/nvme2/yyc/clinicalbert/model/word2vec.model"
    max_seq_length = 2048
    batch_size = 128
    num_epochs = 30
    learning_rate = 5e-4
    patience = 3

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    # -------------------------------
    # 4. åŠ è½½è¯å‘é‡
    # -------------------------------
    print("åŠ è½½ Word2Vec æ¨¡å‹...")
    wv_model = gensim.models.KeyedVectors.load(wv_model_path)
    original_vocab = wv_model.wv.index_to_key
    embed_dim = wv_model.vector_size
    word2idx = {"<PAD>": 0, "<UNK>": 1}
    vocab_size = len(original_vocab) + 2
    embedding_matrix = np.zeros((vocab_size, embed_dim), dtype=np.float32)
    embedding_matrix[1] = np.random.normal(scale=0.6, size=(embed_dim,))
    for i, word in enumerate(original_vocab):
        idx = i + 2
        word2idx[word] = idx
        embedding_matrix[idx] = wv_model.wv[word]
    print(f"è¯è¡¨å¤§å°: {vocab_size}, è¯å‘é‡ç»´åº¦: {embed_dim}")

    # -------------------------------
    # 5. æ„å»º Dataset å’Œ DataLoader
    # -------------------------------
    train_dataset = ClinicalTextDataset(train_csv, word2idx, max_seq_length)
    val_dataset = ClinicalTextDataset(val_csv, word2idx, max_seq_length)
    test_dataset = ClinicalTextDataset(test_csv, word2idx, max_seq_length)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    # -------------------------------
    # 6. åˆå§‹åŒ–æ¨¡å‹
    # -------------------------------
    model = TextCNNClassifier(vocab_size, embed_dim, num_labels=1, embedding_matrix=embedding_matrix)
    model = model.to(device)
    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)

    best_val_loss = float("inf")
    epochs_no_improve = 0

    # -------------------------------
    # 7. è®­ç»ƒä¸éªŒè¯
    # -------------------------------
    for epoch in range(num_epochs):
        model.train()
        losses = []
        for step, (input_ids, labels) in enumerate(train_loader):
            input_ids, labels = input_ids.to(device), labels.to(device)
            optimizer.zero_grad()
            loss, logits = model(input_ids, labels)
            loss.backward()
            optimizer.step()
            losses.append(loss.item())
        print(f"Epoch {epoch+1} Train Loss: {np.mean(losses):.4f}")

        # éªŒè¯
        model.eval()
        val_losses, all_labels, all_probs = [], [], []
        with torch.no_grad():
            for input_ids, labels in val_loader:
                input_ids, labels = input_ids.to(device), labels.to(device)
                loss, logits = model(input_ids, labels)
                val_losses.append(loss.item())
                probs = torch.sigmoid(logits).cpu().numpy().flatten()
                all_probs.extend(probs)
                all_labels.extend(labels.cpu().numpy().flatten())
        avg_val_loss = np.mean(val_losses)
        try:
            auroc = roc_auc_score(all_labels, all_probs)
        except:
            auroc = 0.0
        acc = accuracy_score(all_labels, np.array(all_probs) >= 0.5)
        precision, recall, _ = precision_recall_curve(all_labels, all_probs)
        auprc = auc(recall, precision)
        rp80 = max([r for p, r in zip(precision, recall) if p >= 0.8] or [0.0])
        print(f"[éªŒè¯é›†] Loss: {avg_val_loss:.4f}, AUROC: {auroc:.4f}, Acc: {acc:.4f}, AUPRC: {auprc:.4f}, RP80: {rp80:.4f}")

        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            epochs_no_improve = 0
            torch.save(model.state_dict(), "best_textcnn_model.pth")
            print("âœ… æ¨¡å‹å·²ä¿å­˜")
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= patience:
                print("â¹ï¸ éªŒè¯æŸå¤±æ— æå‡ï¼Œæå‰åœæ­¢è®­ç»ƒ")
                break

    # -------------------------------
    # 8. æµ‹è¯•
    # -------------------------------
    print("ğŸ” æµ‹è¯•é˜¶æ®µåŠ è½½æœ€ä½³æ¨¡å‹...")
    model.load_state_dict(torch.load("best_textcnn_model.pth"))
    model.eval()
    all_labels, all_probs = [], []
    with torch.no_grad():
        for input_ids, labels in test_loader:
            input_ids, labels = input_ids.to(device), labels.to(device)
            loss, logits = model(input_ids, labels)
            probs = torch.sigmoid(logits).cpu().numpy().flatten()
            all_probs.extend(probs)
            all_labels.extend(labels.cpu().numpy().flatten())
    test_auroc = roc_auc_score(all_labels, all_probs)
    test_acc = accuracy_score(all_labels, np.array(all_probs) >= 0.5)
    precision, recall, _ = precision_recall_curve(all_labels, all_probs)
    test_auprc = auc(recall, precision)
    test_rp80 = max([r for p, r in zip(precision, recall) if p >= 0.8] or [0.0])
    print(f"[æµ‹è¯•é›†] AUROC: {test_auroc:.4f}, Accuracy: {test_acc:.4f}, AUPRC: {test_auprc:.4f}, RP80: {test_rp80:.4f}")

if __name__ == "__main__":
    main()
